{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "349068d7",
   "metadata": {},
   "source": [
    "# Contextual RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1b83f1",
   "metadata": {},
   "source": [
    "## 1 - Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c9169a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever,BM25Retriever,EnsembleRetriever\n",
    "from langchain_community.document_transformers.embeddings_redundant_filter import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank, DocumentCompressorPipeline\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fb445f",
   "metadata": {},
   "source": [
    "## 2 - OpenAI API Key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "913c1f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api key\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8fd6ec",
   "metadata": {},
   "source": [
    "## 3 - Crear Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39549af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorDB:\n",
    "\n",
    "\n",
    "    def __init__(self, collection_name: str):\n",
    "        \"\"\"\n",
    "        Inicializar los atributos:\n",
    "        \n",
    "        + collection_name: str, nombre de la coleccion de la base de datos\n",
    "        + text_splitter: RecursiveCharacterTextSplitter, objecto para crear chunks desde documento\n",
    "        + embeddings: OpenAIEmbeddings, modelo para embeddings\n",
    "        + llm: ChatOpenAI, modelo gpt-4o\n",
    "        \"\"\"\n",
    "        \n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=800,\n",
    "                                                            chunk_overlap=100)\n",
    "        \n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "\n",
    "        \n",
    "        self.llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def process_document(self, file_paths: str) -> List[Document]:\n",
    "        \n",
    "        \"\"\"\n",
    "        Procesar cada documento dividiendolo en chunks y generando contexto para cada uno\n",
    "        \n",
    "        Params:\n",
    "        file_paths: lista de strings, rutas a los archivos pdf o txt \n",
    "        \n",
    "        Return:\n",
    "        lista de chunks\n",
    "        \"\"\"\n",
    "        \n",
    "        contextualized_chunks = []\n",
    "        \n",
    "        \n",
    "        for file_path in file_paths:\n",
    "        \n",
    "        \n",
    "            # procesar pdf \n",
    "            if file_path.endswith('.pdf'):\n",
    "\n",
    "                loader = PyPDFLoader(file_path)\n",
    "\n",
    "                pages = loader.load()\n",
    "\n",
    "                for i in tqdm(range(0, len(pages)-2, 1), leave=False, desc='Chunking PDF file'):\n",
    "\n",
    "                    document = pages[i].page_content + pages[i+1].page_content + pages[i+2].page_content\n",
    "\n",
    "                    chunks = self.text_splitter.create_documents([pages[i+1].page_content])\n",
    "\n",
    "                    chunk_with_context = self._generate_contextualized_chunks(document, chunks, file_path)\n",
    "                    \n",
    "                    contextualized_chunks += chunk_with_context\n",
    "\n",
    "\n",
    "            # procesar txt \n",
    "            elif file_path.endswith('.txt'):\n",
    "\n",
    "                with open(file_path, 'r') as file:\n",
    "\n",
    "                    document = file.read()\n",
    "\n",
    "                    chunks = self.text_splitter.create_documents([document])\n",
    "\n",
    "                    chunk_with_context = self._generate_contextualized_chunks(document, chunks, file_path)\n",
    "\n",
    "                    contextualized_chunks += chunk_with_context\n",
    "\n",
    "                \n",
    "        return contextualized_chunks\n",
    "    \n",
    "    \n",
    "    def _generate_contextualized_chunks(self, document: str, chunks: List[Document], file_path: str) -> List[Document]:\n",
    "        \n",
    "        \"\"\"\n",
    "        Genera versiones contextualizadas de los chunks dados.\n",
    "        \n",
    "        Params:\n",
    "        document: str, documento completo para recuperar contexto\n",
    "        chunks: lista de chunks sin contexto\n",
    "        file_path: str, ruta del archivo original\n",
    "        \n",
    "        Return:\n",
    "        lista de chunks contextualizados\n",
    "        \"\"\"\n",
    "        \n",
    "        contextualized_chunks = []\n",
    "        \n",
    "        for chunk in tqdm(chunks, leave=False, desc='Generating chunk context'):\n",
    "            \n",
    "            # crear contexto\n",
    "            context = self._generate_context(document, chunk.page_content)\n",
    "            \n",
    "            contextualized_content = f'{context}\\n\\n{chunk.page_content}'\n",
    "            \n",
    "            # traducir al castellano el chunk \n",
    "            contextualized_content = self._translate_chunks(contextualized_content)\n",
    "            \n",
    "            # a√±adir fuente source\n",
    "            source = file_path.split('/')[-1].split('.')[0].replace('_', ' ').title()\n",
    "            contextualized_content = f'<documento> FUENTE: {source}. '+contextualized_content+'<documento>'\n",
    "\n",
    "            \n",
    "            contextualized_chunks.append(Document(page_content=contextualized_content, \n",
    "                                                  metadata=chunk.metadata))\n",
    "        \n",
    "        return contextualized_chunks\n",
    "    \n",
    "    \n",
    "    def _generate_context(self, document: str, chunk: str) -> str:\n",
    "        \n",
    "        \"\"\"\n",
    "        Genera contexto para un chunk especifico usando un llm. \n",
    "        \n",
    "        Params:\n",
    "        document: str, documento complete para sacar contexto\n",
    "        chunks: chunk sin contexto\n",
    "        \n",
    "        Return:\n",
    "        str, chunk con contexto\n",
    "        \"\"\"\n",
    "        \n",
    "        system_prompt = '''You are an AI assistant specializing in design systems. \n",
    "                           Your task is to provide brief, relevant context for a chunk of text \n",
    "                           from the document provided.\n",
    "                           Here is the document:\n",
    "                           <document>\n",
    "                           {document}\n",
    "                           </document>\n",
    "\n",
    "                           Here is the chunk we want to situate within the whole document::\n",
    "                           <chunk>\n",
    "                           {chunk}\n",
    "                           </chunk>\n",
    "\n",
    "                           Provide a concise context (2-3 sentences) for this chunk, considering the \n",
    "                           following guidelines:\n",
    "                           \n",
    "                           1. Do not use phrases like \"This chunk discusses\", \"The chunk focuses\"\n",
    "                              ,\"This section provides\", or any other reference to summaring. \n",
    "                              Avoid any reference to summaring. Instead, directly state the context.\n",
    "                              Just give the context.\n",
    "                              Do not use phrases like \"This chunk discusses\" or \"This section provides\". \n",
    "                              Instead, directly state the context.\n",
    "\n",
    "                           \n",
    "                           2. Identify the main topic or metric discussed (e.g., archetypes, dynamics, \n",
    "                              hierarchy, system).\n",
    "                           \n",
    "                           3. Mention any relevant time periods or comparisons.\n",
    "                           \n",
    "                           4. If applicable, note how this information relates to design, strategy, \n",
    "                              or market position.\n",
    "                           \n",
    "                           5. Include any key figures or percentages that provide important context.\n",
    "                           \n",
    "\n",
    "                           Please give a short succinct context to situate this chunk within the overall \n",
    "                           document for the purposes of improving search retrieval of the chunk. \n",
    "                           Answer only with the succinct context and nothing else.\n",
    "\n",
    "                           Context:\n",
    "                           '''\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(system_prompt)\n",
    "        \n",
    "        messages = prompt.format_messages(document=document, chunk=chunk)\n",
    "        \n",
    "        response = self.llm.invoke(messages).content\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _translate_chunks(self, chunk: str) -> str:\n",
    "        \n",
    "        \"\"\"\n",
    "        Traducir a castellano todos los chunks.\n",
    "        \n",
    "        Params:\n",
    "        chunk: str, chunk sin traducir\n",
    "        \n",
    "        Return:\n",
    "        str, chunk en castellano\n",
    "        \"\"\"\n",
    "        \n",
    "        system_prompt = '''You are a good translator to spanish.\n",
    "                           Given the next chunk translate to spanish:\n",
    "                           \n",
    "                           <chunk>\n",
    "                           {chunk}\n",
    "                           </chunk>\n",
    "                           \n",
    "                           Just give the traslation, do not comment anything.\n",
    "                           If the chunk is already in spanish, repeat the chunk.\n",
    "                           '''\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(system_prompt)\n",
    "        \n",
    "        messages = prompt.format_messages(chunk=chunk)\n",
    "\n",
    "        response = self.llm.invoke(messages)\n",
    "            \n",
    "        return response.content\n",
    "    \n",
    "    \n",
    "    \n",
    "    def create_vectorstore(self, chunks: List[Document]) -> None:\n",
    "        \n",
    "        \"\"\"\n",
    "        Crea una DB vectorial de Chroma para guardar los chunks.\n",
    "        \n",
    "        Params:\n",
    "        chunks: lista de chunks para guardar\n",
    "        \n",
    "        Return:\n",
    "        None\n",
    "        \"\"\"\n",
    "        \n",
    "        vectordb =  Chroma.from_documents(chunks, \n",
    "                                          self.embeddings, \n",
    "                                          persist_directory='../data/chroma_db',\n",
    "                                          collection_name=self.collection_name)\n",
    "    \n",
    "       \n",
    "    \n",
    "    def create_bm25_retriever(self, chunks: List[Document]) -> None:\n",
    "        \n",
    "        \"\"\"\n",
    "        Crea un retriever BM25 para los chunks dados.\n",
    "        \n",
    "        Params:\n",
    "        chunks: lista de chunks para guardar\n",
    "        \n",
    "        Return:\n",
    "        None\n",
    "        \"\"\"\n",
    "        \n",
    "        bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "        \n",
    "        \n",
    "        # guardar objeto bm25 \n",
    "        with open(f'../data/{self.collection_name}_bm25', 'wb') as bm25_file:\n",
    "            pickle.dump(bm25_retriever, bm25_file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def store_to_db(self, file_paths: list) -> None:\n",
    "        \n",
    "        \"\"\"\n",
    "        Proceso completo de guardado en DB desde documento.\n",
    "        \n",
    "        Params:\n",
    "        file_paths: lista de strings, rutas de los archivos para guardar en Chroma y BM25\n",
    "        \n",
    "        Return:\n",
    "        None\n",
    "        \"\"\"\n",
    "                \n",
    "        chunks = self.process_document(file_paths)\n",
    "                \n",
    "        vectorstore = self.create_vectorstore(chunks)\n",
    "        \n",
    "        bm25 = self.create_bm25_retriever(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c91a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = VectorDB('design')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad6d0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking PDF file:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 181/194 [1:24:18<05:07, 23.62s/it]"
     ]
    }
   ],
   "source": [
    "vectordb.store_to_db(['../data/thinking_systems_from_donella_meadows.pdf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98899bf",
   "metadata": {},
   "source": [
    "## 4 - Recuperar desde VectorDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490c13f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_retriever(collection_name: str) -> ContextualCompressionRetriever:\n",
    "    \n",
    "    \"\"\"\n",
    "    Recuperaci√≥n desde ChromaDB y BM25.\n",
    "    \n",
    "    Params:\n",
    "    collection_name: str, coleccion a ser usada \n",
    "\n",
    "    Return:\n",
    "    ContextualCompressionRetriever, ChromaDB+BM25+Filter+ReRanker \n",
    "    \"\"\"\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    # carga chromaDB\n",
    "    retriver_chroma = Chroma(persist_directory='../data/chroma_db',\n",
    "                             collection_name=collection_name, \n",
    "                             embedding_function=embeddings)\n",
    "    \n",
    "    retriver_chroma = retriver_chroma.as_retriever(search_type='mmr', search_kwargs={'k':20, \n",
    "                                                                                     'lambda_mult': 0.5})\n",
    "    \n",
    "    \n",
    "    # carga BM25\n",
    "    with open(f'../data/{collection_name}_bm25', 'rb') as bm25_file:\n",
    "        bm25_retriever = pickle.load(bm25_file)\n",
    "            \n",
    "    \n",
    "    bm25_retriever.k = 10\n",
    "        \n",
    "    ensemble_retriever = EnsembleRetriever(retrievers=[retriver_chroma, bm25_retriever],\n",
    "                                           weights=[0.5, 0.5])\n",
    "\n",
    "    redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "\n",
    "    reranker = FlashrankRerank()\n",
    "\n",
    "    pipeline_compressor = DocumentCompressorPipeline(transformers=[redundant_filter, reranker])\n",
    "\n",
    "    compression_pipeline = ContextualCompressionRetriever(base_compressor=pipeline_compressor,\n",
    "                                                          base_retriever=ensemble_retriever)\n",
    "\n",
    "    return compression_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd177260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 225 ms, sys: 85.7 ms, total: 311 ms\n",
      "Wall time: 319 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "retriever = ensemble_retriever('design')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87c5295d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.5 s, sys: 2.35 s, total: 31.9 s\n",
      "Wall time: 9.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "response = retriever.invoke('¬øque es un sistema complejo?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c341a2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3628f749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<documento> FUENTE: Thinking Systems From Donella Meadows. <chunk>\\nEl fragmento profundiza en el concepto de sistemas, enfatizando que un sistema es un conjunto interconectado de elementos organizados para lograr un prop√≥sito espec√≠fico, con ejemplos como el sistema digestivo que ilustran esta definici√≥n. Destaca la complejidad y las interrelaciones dentro de los sistemas, sugiriendo que entender un sistema requiere reconocer sus elementos, interconexiones y funci√≥n. Esta comprensi√≥n es crucial para el dise√±o y la estrategia, ya que informa c√≥mo los sistemas pueden ser organizados y gestionados eficazmente para lograr los resultados deseados.\\n\\n/emdash.cap UNO /emdash.cap \\nLos Fundamentos\\nA√∫n no he visto ning√∫n problema, por complicado que sea, que, cuando se \\nmira de la manera correcta, no se vuelva a√∫n m√°s complicado.\\n‚ÄîPoul Anderson1 \\nM√°s que la Suma de sus Partes\\nUn sistema no es solo cualquier colecci√≥n de cosas. Un sistema* es un conjunto \\ninterconectado de elementos que est√° organizado de manera coherente para \\nlograr algo. Si observas esa definici√≥n detenidamente por un minuto, puedes ver \\nque un sistema debe consistir en tres tipos de cosas: elementos, interconexiones \\ny una funci√≥n o prop√≥sito.\\nPor ejemplo, los elementos de tu sistema digestivo incluyen dientes, \\nenzimas, est√≥mago e intestinos. Est√°n interrelacionados a trav√©s del flujo \\nf√≠sico de alimentos y a trav√©s de un elegante conjunto de se√±ales qu√≠micas reguladoras.\\n</chunk><documento>',\n",
       " '<documento> FUENTE: Thinking Systems From Donella Meadows. El documento explora el concepto de pensamiento sist√©mico, utilizando la met√°fora de un zool√≥gico para ilustrar la variedad y complejidad de los sistemas. Se enfatiza que, aunque los sistemas pueden ser categorizados para su estudio, naturalmente interact√∫an y se conectan en ecosistemas din√°micos. Esta secci√≥n prepara el terreno para comprender los componentes individuales del sistema antes de profundizar en su interconexi√≥n y complejidad.\\n\\nel mundo, pero est√° lejos de ser una representaci√≥n completa de esa variedad. Agrupa a los animales por familia: monos aqu√≠, osos all√° (sistemas de una sola acci√≥n aqu√≠, sistemas de dos acciones all√°), para que puedas observar los comportamientos caracter√≠sticos de los monos, en contraposici√≥n a los osos. Pero, al igual que un zool√≥gico, esta colecci√≥n es demasiado ordenada. Para hacer visibles y comprensibles a los animales, los separa entre s√≠ y de su entorno normal que los oculta. As√≠ como los animales de zool√≥gico ocurren m√°s naturalmente mezclados en ecosistemas, los animales de sistemas descritos aqu√≠ normalmente se conectan e interact√∫an entre s√≠ y con otros no ilustrados aqu√≠, todos formando la complejidad zumbante, ululante, chirriante y cambiante en la que vivimos. Los ecosistemas vienen despu√©s. Por el momento, veamos un animal de sistema.<documento>',\n",
       " '<documento> FUENTE: Thinking Systems From Donella Meadows. El texto aborda la complejidad inherente y la no linealidad de los sistemas, enfatizando que los intentos de simplificarlos por conveniencia son a menudo poco pr√°cticos. Destaca la necesidad de comprender y utilizar la complejidad de los sistemas, al tiempo que reconoce que algunos sistemas est√°n estructurados de maneras que conducen a comportamientos problem√°ticos, denominados sistemas \"perversos\". Estos sistemas a menudo exhiben problemas comunes, que se identifican como arquetipos, y requieren cambios estrat√©gicos para abordar su naturaleza problem√°tica.\\n\\nno son propiedades que puedan o deban cambiarse. El mundo es no lineal. Intentar hacerlo lineal para nuestra conveniencia matem√°tica o administrativa no suele ser una buena idea, incluso cuando es factible, y rara vez es factible. Los l√≠mites dependen del problema, son evanescentes y desordenados; tambi√©n son necesarios para la organizaci√≥n y la claridad. Estar menos sorprendido por los sistemas complejos es principalmente una cuesti√≥n de aprender a esperar, apreciar y utilizar la complejidad del mundo. Pero algunos sistemas son m√°s que sorprendentes. Son perversos. Estos son los sistemas que est√°n estructurados de maneras que producen comportamientos verdaderamente problem√°ticos; nos causan grandes problemas. Hay muchas formas de problemas de sistemas, algunas de ellas √∫nicas, pero muchas sorprendentemente comunes. Llamamos a los<documento>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.page_content for e in response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e66dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "for i, e in enumerate(response, 1):\n",
    "    display(Markdown(f\"**Fragmento {i}:**\\n\\n{e.page_content.replace('<documento>', '').replace('</chunk>', '').replace('<chunk>', '---')}\\n\\n---\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "283px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
